{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col, size\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from bio_spark.io.fasta_reader import FASTAReader, FASTAQReader\n",
    "import collections\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre este Notebook\n",
    "\n",
    "Este notebook executa uma clusterização de seuência de Aminoácidos usando a ML lib dp Spark. Clustrização é um método que pode auxiliar os pesquisadores a descobrir relações filogenéticas e/ou relações de similaridade entre sequências sem a necessidade de comparar com uma base de referência. O fluxo é composto dos seguintes passos:\n",
    "\n",
    "1. Leutra e parsing do arquivos fasta de entrada\n",
    "2. Cálculo dos Kmers a partir das sequências encontradas nos arquivos de entrada\n",
    "3. Uso do método de Elbow para encontrar clusters coesos.\n",
    "\n",
    "___\n",
    "\n",
    "## Cluster local\n",
    "\n",
    "Para fins de desenvolvimento, utilizamos imagens Docker para criar um cluster spark local. Esse cluster deve estar rodadndo para que o notebook funcione como esperado. Na raiz do projeto:\n",
    "\n",
    "```shell\n",
    "docker-compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sConf = SparkConf(\"spark://localhost:7077\")\n",
    "sc = SparkContext(conf=sConf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Tdoso os arquivos de entrada serão tratados em único Dataframe\n",
    "\n",
    "```shell\n",
    "INPUT_DIR_PATH: caminho para o diretório com os arquivs .fna (FASTA)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process : 11\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR_PATH = Path(\"/home/thiago/Dados/sparkAAI-1/data/genomes/\")\n",
    "files_to_process = [str(f) for f in INPUT_DIR_PATH.iterdir()]\n",
    "print(\"Files to process :\", len(files_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw file lines to process 15995\n"
     ]
    }
   ],
   "source": [
    "# fasta_plain_df = sc.textFile(','.join(files_to_process))\\/\n",
    "fasta_plain_df = sc.textFile(\"/home/thiago/Dados/sparkAAI-1/data/genomes/Prochlorococcus_sp_W2_genomic.fna\")\\\n",
    "            .map(lambda x: Row(row=x))\\\n",
    "            .zipWithIndex()\\\n",
    "            .toDF([\"row\",\"idx\"])\n",
    "\n",
    "print(\"raw file lines to process\", fasta_plain_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecionando o dataframe lido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                 row|idx|\n",
      "+--------------------+---+\n",
      "|[>ALPB01000001.1 ...|  0|\n",
      "|[GACACTCATCCAATTT...|  1|\n",
      "|[AGAAAAAAATTTACTC...|  2|\n",
      "|[GAACTGATATTGCTAA...|  3|\n",
      "|[GCCAGATATGGAGAAG...|  4|\n",
      "|[CATACCTATTATCGAG...|  5|\n",
      "|[CAAATTTTATTTTGTC...|  6|\n",
      "|[GCCGAACTAGATCCAA...|  7|\n",
      "|[AGGAAAAATTGATAGA...|  8|\n",
      "|[TGGGTTTTGAAATTAA...|  9|\n",
      "|[TGGGTTGGTCCAACAC...| 10|\n",
      "|[TGATCCTGTTGGAGAA...| 11|\n",
      "|[TGAATCTGAAAGCCCT...| 12|\n",
      "|[CGAAAATGCCATGTTA...| 13|\n",
      "|[TATAGGTAAAATCGGA...| 14|\n",
      "|[AAGCAGAAATAGTTGT...| 15|\n",
      "|[GAAGTTAAATTTATTG...| 16|\n",
      "|[>ALPB01000002.1 ...| 17|\n",
      "|[CATTTCTTTAGGTATT...| 18|\n",
      "|[AACTCAATCAATTTGA...| 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_plain_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse dos arquivos FASTA\n",
    "\n",
    "os arquivos [FASTA]([FASTA](https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=BlastHelp)), tem o seguinte formato:\n",
    "\n",
    "```\n",
    ">ID.CONTIG\n",
    "ATTC....\n",
    "GCG...\n",
    "CCG...\n",
    ">ID2.CONTIG\n",
    "GGC...\n",
    "...\n",
    "```\n",
    "\n",
    "nesta primeira sessão fazermos um parse desses arquivos para agrupar as sequẽncias por ID, calcular os kmers para esses contigs e obter um map com as freqências dos kmers em todos os contigs de uma sequẽncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta_id_line(l):\n",
    "    \"\"\"\n",
    "    Desejamos extrair os IDs das sequências da linhas que começarem pelo caracter ''>'. Pelo padrão\n",
    "    FASTA, o ID é a primeira palavra e é um campo composto por ID.CONTIG\n",
    "    \n",
    "    Input>\n",
    "        l: Uma linha de um arquivo FASTA\n",
    "    Return:\n",
    "        ID: da sequência ignorando o número de contigs, ou None caso não seja uma linha de ID\n",
    "    \"\"\"\n",
    "    if l[0][0] == \">\":\n",
    "        heaer_splits = l[0][1:].split(\" \")[0]\n",
    "        seq_id_split = heaer_splits.split(\".\")\n",
    "        return seq_id_split[0]\n",
    "    else:\n",
    "        return None\n",
    "seq2kmer_udf = udf(parse_fasta_id_line, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_null_ids_df = fasta_plain_df.withColumn(\"seqID_wNull\", seq2kmer_udf(\"row\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecionar o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+\n",
      "|                 row|idx| seqID_wNull|\n",
      "+--------------------+---+------------+\n",
      "|[>ALPB01000001.1 ...|  0|ALPB01000001|\n",
      "|[GACACTCATCCAATTT...|  1|        null|\n",
      "|[AGAAAAAAATTTACTC...|  2|        null|\n",
      "|[GAACTGATATTGCTAA...|  3|        null|\n",
      "|[GCCAGATATGGAGAAG...|  4|        null|\n",
      "|[CATACCTATTATCGAG...|  5|        null|\n",
      "|[CAAATTTTATTTTGTC...|  6|        null|\n",
      "|[GCCGAACTAGATCCAA...|  7|        null|\n",
      "|[AGGAAAAATTGATAGA...|  8|        null|\n",
      "|[TGGGTTTTGAAATTAA...|  9|        null|\n",
      "|[TGGGTTGGTCCAACAC...| 10|        null|\n",
      "|[TGATCCTGTTGGAGAA...| 11|        null|\n",
      "|[TGAATCTGAAAGCCCT...| 12|        null|\n",
      "|[CGAAAATGCCATGTTA...| 13|        null|\n",
      "|[TATAGGTAAAATCGGA...| 14|        null|\n",
      "|[AAGCAGAAATAGTTGT...| 15|        null|\n",
      "|[GAAGTTAAATTTATTG...| 16|        null|\n",
      "|[>ALPB01000002.1 ...| 17|ALPB01000002|\n",
      "|[CATTTCTTTAGGTATT...| 18|        null|\n",
      "|[AACTCAATCAATTTGA...| 19|        null|\n",
      "+--------------------+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_null_ids_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "número de seuências para serem processadas 108\n"
     ]
    }
   ],
   "source": [
    "num_ids = fasta_null_ids_df.where(F.col(\"seqID_wNull\").isNotNull()).count()\n",
    "print(\"número de seuências para serem processadas\", num_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desejamos fazer um \"fillna\" com o último valor não nulo encontrado na coluna de sequência, para isso usaremos um operador de janela deslizante em cima do índice que serve para manter a ordem original das linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_n_filter_df = fasta_null_ids_df.withColumn(\n",
    "    \"seqID\", F.last('seqID_wNull', ignorenulls=True)\\\n",
    "    .over(Window\\\n",
    "    .orderBy('idx')\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir devemos excluir as linhas de header e renomear as colunas excluíndo as que não foram utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+------------+\n",
      "|                 row|idx| seqID_wNull|       seqID|\n",
      "+--------------------+---+------------+------------+\n",
      "|[>ALPB01000001.1 ...|  0|ALPB01000001|ALPB01000001|\n",
      "|[GACACTCATCCAATTT...|  1|        null|ALPB01000001|\n",
      "|[AGAAAAAAATTTACTC...|  2|        null|ALPB01000001|\n",
      "|[GAACTGATATTGCTAA...|  3|        null|ALPB01000001|\n",
      "|[GCCAGATATGGAGAAG...|  4|        null|ALPB01000001|\n",
      "|[CATACCTATTATCGAG...|  5|        null|ALPB01000001|\n",
      "|[CAAATTTTATTTTGTC...|  6|        null|ALPB01000001|\n",
      "|[GCCGAACTAGATCCAA...|  7|        null|ALPB01000001|\n",
      "|[AGGAAAAATTGATAGA...|  8|        null|ALPB01000001|\n",
      "|[TGGGTTTTGAAATTAA...|  9|        null|ALPB01000001|\n",
      "|[TGGGTTGGTCCAACAC...| 10|        null|ALPB01000001|\n",
      "|[TGATCCTGTTGGAGAA...| 11|        null|ALPB01000001|\n",
      "|[TGAATCTGAAAGCCCT...| 12|        null|ALPB01000001|\n",
      "|[CGAAAATGCCATGTTA...| 13|        null|ALPB01000001|\n",
      "|[TATAGGTAAAATCGGA...| 14|        null|ALPB01000001|\n",
      "|[AAGCAGAAATAGTTGT...| 15|        null|ALPB01000001|\n",
      "|[GAAGTTAAATTTATTG...| 16|        null|ALPB01000001|\n",
      "|[>ALPB01000002.1 ...| 17|ALPB01000002|ALPB01000002|\n",
      "|[CATTTCTTTAGGTATT...| 18|        null|ALPB01000002|\n",
      "|[AACTCAATCAATTTGA...| 19|        null|ALPB01000002|\n",
      "+--------------------+---+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_n_filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_df = fasta_n_filter_df\\\n",
    "                .where(F.col(\"seqID_wNull\").isNull())\\\n",
    "                .select(\"seqID\",\"row\")\\\n",
    "                .toDF(\"seqID\",\"seq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Dataframe tratado tem o seguinte esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- seq: struct (nullable = true)\n",
      " |    |-- row: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspeção do daframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_per_seq_df = fasta_df.rdd\\\n",
    "            .map(lambda r: (r.seqID, r.seq[0]))\\\n",
    "            .reduceByKey(lambda x,y:x+y)\\\n",
    "            .map(lambda x: Row(seqID=x[1],seq=x[0]))\\\n",
    "            .toDF([\"seqID\", \"seq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- seq: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_per_seq_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|       seqID|                 seq|\n",
      "+------------+--------------------+\n",
      "|ALPB01000001|GACACTCATCCAATTTT...|\n",
      "|ALPB01000002|CATTTCTTTAGGTATTG...|\n",
      "|ALPB01000003|TTAAGTAATGTAGTACC...|\n",
      "|ALPB01000004|GAACCATTAAAGGGGCT...|\n",
      "|ALPB01000005|GTAAGCATTGTCATTTA...|\n",
      "|ALPB01000006|GCCCAACCATTAAAACG...|\n",
      "|ALPB01000007|TTCGCCTTTTAAGTAAT...|\n",
      "|ALPB01000008|TATTGAAGAAGGGACTT...|\n",
      "|ALPB01000009|CTTTCTCCAATTAAAAC...|\n",
      "|ALPB01000010|GAGAAGTTGTAAATACA...|\n",
      "|ALPB01000011|GAGCATAATATACCTCC...|\n",
      "|ALPB01000012|TTTAAATTGATATTATT...|\n",
      "|ALPB01000013|TTAATTTATTATTTTCA...|\n",
      "|ALPB01000014|CATTAATGCTTGGCTCG...|\n",
      "|ALPB01000015|ATTTTGTTTTATCCTTT...|\n",
      "|ALPB01000016|CGTTGCACCTTTTGAAT...|\n",
      "|ALPB01000017|CCTAAATGCACAAAAGA...|\n",
      "|ALPB01000018|TTATGAAAGAAGATTTT...|\n",
      "|ALPB01000019|AGTTTACACAATTATAA...|\n",
      "|ALPB01000020|TTCGGATTTGTTGGATC...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_per_seq_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Kmers\n",
    "\n",
    "Nesta sessão faremos o cálculo dos [kmers](https://en.wikipedia.org/wiki/K-mer) de tambo ```K```. O objetivo é associar cada ID de sequência ao conjunto de kmers distiontos presentes em todos os seus motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2kmerTy = T.ArrayType(T.StringType())\n",
    "def seq2kmer(seq_):\n",
    "    global K\n",
    "    value = seq_.strip()\n",
    "    num_kmers = len(value) - K + 1\n",
    "    kmers_list = [value[n*K:K*(n+1)] for n in range(0, num_kmers)]\n",
    "    \n",
    "    # return len(value)\n",
    "    return kmers_list\n",
    "\n",
    "seq2kmer_udf = udf(seq2kmer,Seq2kmerTy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_kmers_df = fasta_per_seq_df\\\n",
    "        .withColumn(\"kmers\", seq2kmer_udf(\"seq\"))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- seq: string (nullable = true)\n",
      " |-- kmers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_kmers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|       seqID|                 seq|               kmers|\n",
      "+------------+--------------------+--------------------+\n",
      "|ALPB01000001|GACACTCATCCAATTTT...|[GAC, ACT, CAT, C...|\n",
      "|ALPB01000002|CATTTCTTTAGGTATTG...|[CAT, TTC, TTT, A...|\n",
      "|ALPB01000003|TTAAGTAATGTAGTACC...|[TTA, AGT, AAT, G...|\n",
      "|ALPB01000004|GAACCATTAAAGGGGCT...|[GAA, CCA, TTA, A...|\n",
      "|ALPB01000005|GTAAGCATTGTCATTTA...|[GTA, AGC, ATT, G...|\n",
      "|ALPB01000006|GCCCAACCATTAAAACG...|[GCC, CAA, CCA, T...|\n",
      "|ALPB01000007|TTCGCCTTTTAAGTAAT...|[TTC, GCC, TTT, T...|\n",
      "|ALPB01000008|TATTGAAGAAGGGACTT...|[TAT, TGA, AGA, A...|\n",
      "|ALPB01000009|CTTTCTCCAATTAAAAC...|[CTT, TCT, CCA, A...|\n",
      "|ALPB01000010|GAGAAGTTGTAAATACA...|[GAG, AAG, TTG, T...|\n",
      "|ALPB01000011|GAGCATAATATACCTCC...|[GAG, CAT, AAT, A...|\n",
      "|ALPB01000012|TTTAAATTGATATTATT...|[TTT, AAA, TTG, A...|\n",
      "|ALPB01000013|TTAATTTATTATTTTCA...|[TTA, ATT, TAT, T...|\n",
      "|ALPB01000014|CATTAATGCTTGGCTCG...|[CAT, TAA, TGC, T...|\n",
      "|ALPB01000015|ATTTTGTTTTATCCTTT...|[ATT, TTG, TTT, T...|\n",
      "|ALPB01000016|CGTTGCACCTTTTGAAT...|[CGT, TGC, ACC, T...|\n",
      "|ALPB01000017|CCTAAATGCACAAAAGA...|[CCT, AAA, TGC, A...|\n",
      "|ALPB01000018|TTATGAAAGAAGATTTT...|[TTA, TGA, AAG, A...|\n",
      "|ALPB01000019|AGTTTACACAATTATAA...|[AGT, TTA, CAC, A...|\n",
      "|ALPB01000020|TTCGGATTTGTTGGATC...|[TTC, GGA, TTT, G...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_kmers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspeção do daframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- seq: string (nullable = true)\n",
      " |-- kmers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_kmers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para validação, podemos obter estatísticas básicas dso kmers obtidos. Para isso vamos contar o número de kmers por ID de sequência e obter um describe da coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kmers_df = fasta_kmers_df\\\n",
    "                    .withColumn(\"n_kmers\", size(col(\"kmers\")))\\\n",
    "                    .select(\"n_kmers\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers_pofile_df = fasta_kmers_df.select(\"seqID\",\"kmers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de features\n",
    "\n",
    "O número de K que defie o tamanho dos k-mers define um espaço de features de dimensão $4^K$, para codificar essas features podemos usar a classe ```CountVectorizer```. Essa codificação atribui ordinais a cada kmer único e cria duas listas para representar a presença e o frequência absoluta dos mesmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- kmers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmers_pofile_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.58 ms, sys: 3.25 ms, total: 9.83 ms\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(inputCol=\"kmers\", outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(kmers_pofile_df)\n",
    "\n",
    "features_df = model.transform(kmers_pofile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conferir resultado temporário\n",
    "features_df.select(\"seqID\",\"features\").toPandas().to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de features únicas  108\n",
      "CPU times: user 18.3 ms, sys: 0 ns, total: 18.3 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_features_count = features_df.select(\"features\").distinct().count()\n",
    "print(\"Número de features únicas \",unique_features_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 das 108 sequências tem features únicas\n"
     ]
    }
   ],
   "source": [
    "print(\"%d das %d sequências tem features únicas\" % (unique_features_count, num_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Para o ajuste dos hiperparâmetros da clusterização devemos fazer um parameter sweep para achar o número ideal de clusters. A avaliação da qualidade do cluster é dada pela [Métreica de Silhouette](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/ml/evaluation/ClusteringEvaluator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkm = BisectingKMeans()\n",
    "# model = bkm.fit(features_df)\n",
    "clustering_pipeline = Pipeline(stages=[bkm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 527 ms, total: 1.79 s\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(bkm.k, [5, 10, 20, 50, 70, 100]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=clustering_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=ClusteringEvaluator(),\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel= crossval.fit(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cvModel.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------+\n",
      "|       seqID|               kmers|            features|prediction|\n",
      "+------------+--------------------+--------------------+----------+\n",
      "|ALPB01000001|[GAC, ACT, CAT, C...|(83,[0,1,2,3,4,6,...|        25|\n",
      "|ALPB01000002|[CAT, TTC, TTT, A...|(83,[0,1,2,3,4,5,...|        81|\n",
      "|ALPB01000003|[TTA, AGT, AAT, G...|(83,[0,1,2,3,4,5,...|        45|\n",
      "|ALPB01000004|[GAA, CCA, TTA, A...|(83,[0,1,2,3,4,5,...|        72|\n",
      "|ALPB01000005|[GTA, AGC, ATT, G...|(83,[0,1,2,4,5,6,...|         4|\n",
      "|ALPB01000006|[GCC, CAA, CCA, T...|(83,[0,1,2,3,4,5,...|        56|\n",
      "|ALPB01000007|[TTC, GCC, TTT, T...|(83,[0,1,2,3,4,5,...|        77|\n",
      "|ALPB01000008|[TAT, TGA, AGA, A...|(83,[0,1,2,3,4,5,...|        53|\n",
      "|ALPB01000009|[CTT, TCT, CCA, A...|(83,[0,1,2,3,4,5,...|        55|\n",
      "|ALPB01000010|[GAG, AAG, TTG, T...|(83,[0,1,2,3,4,5,...|        83|\n",
      "|ALPB01000011|[GAG, CAT, AAT, A...|(83,[0,1,2,3,4,5,...|         1|\n",
      "|ALPB01000012|[TTT, AAA, TTG, A...|(83,[0,1,2,3,4,5,...|        78|\n",
      "|ALPB01000013|[TTA, ATT, TAT, T...|(83,[0,1,2,3,4,5,...|        38|\n",
      "|ALPB01000014|[CAT, TAA, TGC, T...|(83,[0,1,2,3,4,5,...|         6|\n",
      "|ALPB01000015|[ATT, TTG, TTT, T...|(83,[0,1,2,3,4,5,...|        61|\n",
      "|ALPB01000016|[CGT, TGC, ACC, T...|(83,[0,1,2,3,4,5,...|        71|\n",
      "|ALPB01000017|[CCT, AAA, TGC, A...|(83,[0,1,2,3,4,5,...|         9|\n",
      "|ALPB01000018|[TTA, TGA, AAG, A...|(83,[0,1,2,3,4,5,...|        57|\n",
      "|ALPB01000019|[AGT, TTA, CAC, A...|(83,[0,1,2,3,4,5,...|        20|\n",
      "|ALPB01000020|[TTC, GGA, TTT, G...|(83,[0,1,2,3,4,5,...|        12|\n",
      "+------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        prediction|\n",
      "+-------+------------------+\n",
      "|  count|               108|\n",
      "|   mean|47.398148148148145|\n",
      "| stddev|29.315218788427305|\n",
      "|    min|                 0|\n",
      "|    max|                99|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.select(\"prediction\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_env",
   "language": "python",
   "name": "bio_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
