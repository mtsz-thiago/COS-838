{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "import pyspark.sql.types as T \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col, size\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from bio_spark.io.fasta_reader import FASTAReader, FASTAQReader\n",
    "import collections\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre este Notebook\n",
    "\n",
    "Este notebook executa uma clusterização de seuência de Aminoácidos usando a ML lib dp Spark. Clustrização é um método que pode auxiliar os pesquisadores a descobrir relações filogenéticas e/ou relações de similaridade entre sequências sem a necessidade de comparar com uma base de referência. O fluxo é composto dos seguintes passos:\n",
    "\n",
    "1. Leutra e parsing do arquivos fasta de entrada\n",
    "2. Cálculo dos Kmers a partir das sequências encontradas nos arquivos de entrada\n",
    "3. Uso do método de Elbow para encontrar clusters coesos.\n",
    "\n",
    "___\n",
    "\n",
    "## Cluster local\n",
    "\n",
    "Para fins de desenvolvimento, utilizamos imagens Docker para criar um cluster spark local. Esse cluster deve estar rodadndo para que o notebook funcione como esperado. Na raiz do projeto:\n",
    "\n",
    "```shell\n",
    "docker-compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sConf = SparkConf(\"spark://localhost:7077\")\n",
    "sc = SparkContext(conf=sConf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Tdoso os arquivos de entrada serão tratados em único Dataframe\n",
    "\n",
    "```shell\n",
    "INPUT_DIR_PATH: caminho para o diretório com os arquivs .fna (FASTA)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process : 10\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR_PATH = Path(\"/home/thiago/Dados/sparkAAI-1/data/genomes/\")\n",
    "OUTPUT_DIR_PATH = Path(\"/home/thiago/Dados/sparkAAI-1/output/\")\n",
    "files_to_process = [str(f) for f in INPUT_DIR_PATH.iterdir()]\n",
    "print(\"Files to process :\", len(files_to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw file lines to process 86243\n"
     ]
    }
   ],
   "source": [
    "fasta_plain_df = sc.textFile(','.join(files_to_process))\\\n",
    "            .map(lambda x: Row(row=x))\\\n",
    "            .zipWithIndex()\\\n",
    "            .toDF([\"row\",\"idx\"])\n",
    "\n",
    "print(\"raw file lines to process\", fasta_plain_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecionando o dataframe lido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                 row|idx|\n",
      "+--------------------+---+\n",
      "|[>ALPH01000001.1 ...|  0|\n",
      "|[TCTCCCAGCACTTAGG...|  1|\n",
      "|[CAACCTCTTTAGAGTT...|  2|\n",
      "|[ATATTAGAAAGTACTT...|  3|\n",
      "|[AATTCCCGCACTTCTT...|  4|\n",
      "|[CAGGACTTGTATCAAG...|  5|\n",
      "|[CCTGCAGTAACACATG...|  6|\n",
      "|[TCTTATTTCTCTCCAA...|  7|\n",
      "|[ATTCTACTTCTTGAAT...|  8|\n",
      "|[CAACCTCCTGTTTTTA...|  9|\n",
      "|[CCACATTAAATCTATA...| 10|\n",
      "|[AATCTTGATTCAATTT...| 11|\n",
      "|[CCACCAAATCTCCTAT...| 12|\n",
      "|[ATCCGTTATATAAATT...| 13|\n",
      "|[GCAAGTCAGGATCTTG...| 14|\n",
      "|[CCTGAGATTGACTTCC...| 15|\n",
      "|[TGTAAATTGATCATTA...| 16|\n",
      "|[CGCCAATAAATTTGAT...| 17|\n",
      "|[AGAAATTTCACCTCTT...| 18|\n",
      "|[TTTAGAAACTTTAATT...| 19|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_plain_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse dos arquivos FASTA\n",
    "\n",
    "os arquivos [FASTA]([FASTA](https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&PAGE_TYPE=BlastDocs&DOC_TYPE=BlastHelp)), tem o seguinte formato:\n",
    "\n",
    "```\n",
    ">ID.CONTIG\n",
    "ATTC....\n",
    "GCG...\n",
    "CCG...\n",
    ">ID2.CONTIG\n",
    "GGC...\n",
    "...\n",
    "```\n",
    "\n",
    "nesta primeira sessão fazermos um parse desses arquivos para agrupar as sequẽncias por ID, calcular os kmers para esses contigs e obter um map com as freqências dos kmers em todos os contigs de uma sequẽncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta_id_line(l):\n",
    "    \"\"\"\n",
    "    Desejamos extrair os IDs das sequências da linhas que começarem pelo caracter ''>'. Pelo padrão\n",
    "    FASTA, o ID é a primeira palavra e é um campo composto por ID.CONTIG\n",
    "    \n",
    "    Input>\n",
    "        l: Uma linha de um arquivo FASTA\n",
    "    Return:\n",
    "        ID: da sequência ignorando o número de contigs, ou None caso não seja uma linha de ID\n",
    "    \"\"\"\n",
    "    if l[0][0] == \">\":\n",
    "        heaer_splits = l[0][1:].split(\" \")[0]\n",
    "        seq_id_split = heaer_splits.split(\".\")\n",
    "        return seq_id_split[0]\n",
    "    else:\n",
    "        return None\n",
    "seq2kmer_udf = udf(parse_fasta_id_line, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_null_ids_df = fasta_plain_df.withColumn(\"seqID_wNull\", seq2kmer_udf(\"row\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecionar o resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+\n",
      "|                 row|idx| seqID_wNull|\n",
      "+--------------------+---+------------+\n",
      "|[>ALPH01000001.1 ...|  0|ALPH01000001|\n",
      "|[TCTCCCAGCACTTAGG...|  1|        null|\n",
      "|[CAACCTCTTTAGAGTT...|  2|        null|\n",
      "|[ATATTAGAAAGTACTT...|  3|        null|\n",
      "|[AATTCCCGCACTTCTT...|  4|        null|\n",
      "|[CAGGACTTGTATCAAG...|  5|        null|\n",
      "|[CCTGCAGTAACACATG...|  6|        null|\n",
      "|[TCTTATTTCTCTCCAA...|  7|        null|\n",
      "|[ATTCTACTTCTTGAAT...|  8|        null|\n",
      "|[CAACCTCCTGTTTTTA...|  9|        null|\n",
      "|[CCACATTAAATCTATA...| 10|        null|\n",
      "|[AATCTTGATTCAATTT...| 11|        null|\n",
      "|[CCACCAAATCTCCTAT...| 12|        null|\n",
      "|[ATCCGTTATATAAATT...| 13|        null|\n",
      "|[GCAAGTCAGGATCTTG...| 14|        null|\n",
      "|[CCTGAGATTGACTTCC...| 15|        null|\n",
      "|[TGTAAATTGATCATTA...| 16|        null|\n",
      "|[CGCCAATAAATTTGAT...| 17|        null|\n",
      "|[AGAAATTTCACCTCTT...| 18|        null|\n",
      "|[TTTAGAAACTTTAATT...| 19|        null|\n",
      "+--------------------+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_null_ids_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "número de seuências para serem processadas 1864\n"
     ]
    }
   ],
   "source": [
    "num_ids = fasta_null_ids_df.where(F.col(\"seqID_wNull\").isNotNull()).count()\n",
    "print(\"número de seuências para serem processadas\", num_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desejamos fazer um \"fillna\" com o último valor não nulo encontrado na coluna de sequência, para isso usaremos um operador de janela deslizante em cima do índice que serve para manter a ordem original das linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_n_filter_df = fasta_null_ids_df.withColumn(\n",
    "    \"seqID\", F.last('seqID_wNull', ignorenulls=True)\\\n",
    "    .over(Window\\\n",
    "    .orderBy('idx')\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir devemos excluir as linhas de header e renomear as colunas excluíndo as que não foram utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+------------+------------+\n",
      "|                 row|idx| seqID_wNull|       seqID|\n",
      "+--------------------+---+------------+------------+\n",
      "|[>ALPH01000001.1 ...|  0|ALPH01000001|ALPH01000001|\n",
      "|[TCTCCCAGCACTTAGG...|  1|        null|ALPH01000001|\n",
      "|[CAACCTCTTTAGAGTT...|  2|        null|ALPH01000001|\n",
      "|[ATATTAGAAAGTACTT...|  3|        null|ALPH01000001|\n",
      "|[AATTCCCGCACTTCTT...|  4|        null|ALPH01000001|\n",
      "|[CAGGACTTGTATCAAG...|  5|        null|ALPH01000001|\n",
      "|[CCTGCAGTAACACATG...|  6|        null|ALPH01000001|\n",
      "|[TCTTATTTCTCTCCAA...|  7|        null|ALPH01000001|\n",
      "|[ATTCTACTTCTTGAAT...|  8|        null|ALPH01000001|\n",
      "|[CAACCTCCTGTTTTTA...|  9|        null|ALPH01000001|\n",
      "|[CCACATTAAATCTATA...| 10|        null|ALPH01000001|\n",
      "|[AATCTTGATTCAATTT...| 11|        null|ALPH01000001|\n",
      "|[CCACCAAATCTCCTAT...| 12|        null|ALPH01000001|\n",
      "|[ATCCGTTATATAAATT...| 13|        null|ALPH01000001|\n",
      "|[GCAAGTCAGGATCTTG...| 14|        null|ALPH01000001|\n",
      "|[CCTGAGATTGACTTCC...| 15|        null|ALPH01000001|\n",
      "|[TGTAAATTGATCATTA...| 16|        null|ALPH01000001|\n",
      "|[CGCCAATAAATTTGAT...| 17|        null|ALPH01000001|\n",
      "|[AGAAATTTCACCTCTT...| 18|        null|ALPH01000001|\n",
      "|[TTTAGAAACTTTAATT...| 19|        null|ALPH01000001|\n",
      "+--------------------+---+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_n_filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_df = fasta_n_filter_df\\\n",
    "                .where(F.col(\"seqID_wNull\").isNull())\\\n",
    "                .select(\"seqID\",\"row\")\\\n",
    "                .toDF(\"seqID\",\"seq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Dataframe tratado tem o seguinte esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- seq: struct (nullable = true)\n",
      " |    |-- row: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspeção do daframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_per_seq_df = fasta_df.rdd\\\n",
    "            .map(lambda r: (r.seqID, r.seq[0]))\\\n",
    "            .reduceByKey(lambda x,y:x+y)\\\n",
    "            .map(lambda x: Row(seqID=x[1],seq=x[0]))\\\n",
    "            .toDF([\"seqID\", \"seq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- seq: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_per_seq_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|       seqID|                 seq|\n",
      "+------------+--------------------+\n",
      "|ALPH01000001|TCTCCCAGCACTTAGGC...|\n",
      "|ALPH01000002|CCTTGCTTATTTAGAAA...|\n",
      "|ALPH01000003|ATTCTTCTTCATCATCC...|\n",
      "|ALPH01000004|AATATCATTTCTTACTT...|\n",
      "|ALPH01000005|AACTTTTAATTGGCAAA...|\n",
      "|ALPH01000006|CCACTACTAACAATTTC...|\n",
      "|ALPH01000007|CTTGGCTTGTTTTTATC...|\n",
      "|ALPH01000008|CTGAGTCCTATTTAAAT...|\n",
      "|ALPH01000009|CGATGTAATGGCTATGC...|\n",
      "|ALPH01000010|TCTCACTAGAAGAAAAT...|\n",
      "|ALPH01000011|GTTTTTATCAGTAGCTT...|\n",
      "|ALPH01000012|AGGGTGTCGGTTAAAAG...|\n",
      "|ALPH01000013|TTTTCATCTAATAAGTA...|\n",
      "|ALPH01000014|AATGTTGTGAGCTTTAA...|\n",
      "|ALPH01000015|ACTGCAGCATTATTTAT...|\n",
      "|ALPH01000016|GCAATACCTCCAACAAT...|\n",
      "|ALPH01000017|GACTCTGAAAGTAAATA...|\n",
      "|ALPH01000018|AGACTCATTGGACATAT...|\n",
      "|ALPH01000019|CTTCTATATCACTAGCG...|\n",
      "|ALPH01000020|AGGATTTTTTATTTTTA...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_per_seq_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Kmers\n",
    "\n",
    "Nesta sessão faremos o cálculo dos [kmers](https://en.wikipedia.org/wiki/K-mer) de tambo ```K```. O objetivo é associar cada ID de sequência ao conjunto de kmers distiontos presentes em todos os seus motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seq2kmerTy = T.ArrayType(T.StringType())\n",
    "def seq2kmer(seq_):\n",
    "    global K\n",
    "    value = seq_.strip()\n",
    "    num_kmers = len(value) - K + 1\n",
    "    kmers_list = [value[n:K+n] for n in range(0, num_kmers)]\n",
    "    \n",
    "    # return len(value)\n",
    "    return kmers_list\n",
    "\n",
    "seq2kmer_udf = udf(seq2kmer,Seq2kmerTy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_kmers_df = fasta_per_seq_df\\\n",
    "        .withColumn(\"kmers\", seq2kmer_udf(\"seq\"))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- seq: string (nullable = true)\n",
      " |-- kmers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_kmers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|       seqID|                 seq|               kmers|\n",
      "+------------+--------------------+--------------------+\n",
      "|ALPH01000001|TCTCCCAGCACTTAGGC...|[TCT, CTC, TCC, C...|\n",
      "|ALPH01000002|CCTTGCTTATTTAGAAA...|[CCT, CTT, TTG, T...|\n",
      "|ALPH01000003|ATTCTTCTTCATCATCC...|[ATT, TTC, TCT, C...|\n",
      "|ALPH01000004|AATATCATTTCTTACTT...|[AAT, ATA, TAT, A...|\n",
      "|ALPH01000005|AACTTTTAATTGGCAAA...|[AAC, ACT, CTT, T...|\n",
      "|ALPH01000006|CCACTACTAACAATTTC...|[CCA, CAC, ACT, C...|\n",
      "|ALPH01000007|CTTGGCTTGTTTTTATC...|[CTT, TTG, TGG, G...|\n",
      "|ALPH01000008|CTGAGTCCTATTTAAAT...|[CTG, TGA, GAG, A...|\n",
      "|ALPH01000009|CGATGTAATGGCTATGC...|[CGA, GAT, ATG, T...|\n",
      "|ALPH01000010|TCTCACTAGAAGAAAAT...|[TCT, CTC, TCA, C...|\n",
      "|ALPH01000011|GTTTTTATCAGTAGCTT...|[GTT, TTT, TTT, T...|\n",
      "|ALPH01000012|AGGGTGTCGGTTAAAAG...|[AGG, GGG, GGT, G...|\n",
      "|ALPH01000013|TTTTCATCTAATAAGTA...|[TTT, TTT, TTC, T...|\n",
      "|ALPH01000014|AATGTTGTGAGCTTTAA...|[AAT, ATG, TGT, G...|\n",
      "|ALPH01000015|ACTGCAGCATTATTTAT...|[ACT, CTG, TGC, G...|\n",
      "|ALPH01000016|GCAATACCTCCAACAAT...|[GCA, CAA, AAT, A...|\n",
      "|ALPH01000017|GACTCTGAAAGTAAATA...|[GAC, ACT, CTC, T...|\n",
      "|ALPH01000018|AGACTCATTGGACATAT...|[AGA, GAC, ACT, C...|\n",
      "|ALPH01000019|CTTCTATATCACTAGCG...|[CTT, TTC, TCT, C...|\n",
      "|ALPH01000020|AGGATTTTTTATTTTTA...|[AGG, GGA, GAT, A...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta_kmers_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspeção do daframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para validação, podemos obter estatísticas básicas dso kmers obtidos. Para isso vamos contar o número de kmers por ID de sequência e obter um describe da coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_kmers_df = fasta_kmers_df\\\n",
    "                    .withColumn(\"n_kmers\", size(col(\"kmers\")))\\\n",
    "                    .select(\"n_kmers\")\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers_pofile_df = fasta_kmers_df.select(\"seqID\",\"kmers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de features\n",
    "\n",
    "O número de K que defie o tamanho dos k-mers define um espaço de features de dimensão $4^K$, para codificar essas features podemos usar a classe ```CountVectorizer```. Essa codificação atribui ordinais a cada kmer único e cria duas listas para representar a presença e o frequência absoluta dos mesmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- kmers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmers_pofile_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 ms, sys: 0 ns, total: 14.9 ms\n",
      "Wall time: 3.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = CountVectorizer(inputCol=\"kmers\", outputCol=\"features\")\n",
    "\n",
    "model = cv.fit(kmers_pofile_df)\n",
    "\n",
    "features_df = model.transform(kmers_pofile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conferir resultado temporário\n",
    "features_df.select(\"seqID\",\"features\").toPandas().to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de features únicas  1864\n",
      "CPU times: user 16.5 ms, sys: 4.31 ms, total: 20.8 ms\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unique_features_count = features_df.select(\"features\").distinct().count()\n",
    "print(\"Número de features únicas \",unique_features_count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 das 108 sequências tem features únicas\n"
     ]
    }
   ],
   "source": [
    "print(\"%d das %d sequências tem features únicas\" % (unique_features_count, num_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seqID: string (nullable = true)\n",
      " |-- kmers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "|       seqID|               kmers|            features|\n",
      "+------------+--------------------+--------------------+\n",
      "|ALPH01000001|[TCT, CTC, TCC, C...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000002|[CCT, CTT, TTG, T...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000003|[ATT, TTC, TCT, C...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000004|[AAT, ATA, TAT, A...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000005|[AAC, ACT, CTT, T...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000006|[CCA, CAC, ACT, C...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000007|[CTT, TTG, TGG, G...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000008|[CTG, TGA, GAG, A...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000009|[CGA, GAT, ATG, T...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000010|[TCT, CTC, TCA, C...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000011|[GTT, TTT, TTT, T...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000012|[AGG, GGG, GGT, G...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000013|[TTT, TTT, TTC, T...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000014|[AAT, ATG, TGT, G...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000015|[ACT, CTG, TGC, G...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000016|[GCA, CAA, AAT, A...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000017|[GAC, ACT, CTC, T...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000018|[AGA, GAC, ACT, C...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000019|[CTT, TTC, TCT, C...|(82,[0,1,2,3,4,5,...|\n",
      "|ALPH01000020|[AGG, GGA, GAT, A...|(82,[0,1,2,3,4,5,...|\n",
      "+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "salva arquivo temporário com os kmers e suas frequências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseVectorToColumsn(seqID, vector):\n",
    "    global vocab\n",
    "    \n",
    "    vector_as_dict = {vocab[k]:str(v) for k,v in zip(vector.indices, vector.values)}\n",
    "    vector_as_dict[\"seqID\"] = seqID\n",
    "    vector_as_row = Row(**vector_as_dict)\n",
    "    return vector_as_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_kmers_rdd = features_df.rdd\\\n",
    "            .map(lambda r: sparseVectorToColumsn(r.seqID, r.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_kmers_rdd.saveAsTextFile(str(OUTPUT_DIR_PATH.joinpath(\"kmers_freq\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Para o ajuste dos hiperparâmetros da clusterização devemos fazer um parameter sweep para achar o número ideal de clusters. A avaliação da qualidade do cluster é dada pela [Métreica de Silhouette](https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/ml/evaluation/ClusteringEvaluator.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkm = BisectingKMeans()\n",
    "# model = bkm.fit(features_df)\n",
    "clustering_pipeline = Pipeline(stages=[bkm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.53 s, sys: 544 ms, total: 2.08 s\n",
      "Wall time: 6min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(bkm.k, [5, 10, 20, 50, 70, 100]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=clustering_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=ClusteringEvaluator(),\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel= crossval.fit(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cvModel.transform(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pandasDF = cluster_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pandasDF.to_csv(str(OUTPUT_DIR_PATH.joinpath(\"result_cluster.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(str(OUTPUT_DIR_PATH.joinpath(\"cluster_model\")),\"w\") as f:\n",
    "#     pickle.dumps(cvModel, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------+\n",
      "|       seqID|               kmers|            features|prediction|\n",
      "+------------+--------------------+--------------------+----------+\n",
      "|ALPH01000001|[TCT, CTC, TCC, C...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000002|[CCT, CTT, TTG, T...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000003|[ATT, TTC, TCT, C...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000004|[AAT, ATA, TAT, A...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000005|[AAC, ACT, CTT, T...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000006|[CCA, CAC, ACT, C...|(82,[0,1,2,3,4,5,...|         1|\n",
      "|ALPH01000007|[CTT, TTG, TGG, G...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000008|[CTG, TGA, GAG, A...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000009|[CGA, GAT, ATG, T...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000010|[TCT, CTC, TCA, C...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000011|[GTT, TTT, TTT, T...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000012|[AGG, GGG, GGT, G...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000013|[TTT, TTT, TTC, T...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000014|[AAT, ATG, TGT, G...|(82,[0,1,2,3,4,5,...|         1|\n",
      "|ALPH01000015|[ACT, CTG, TGC, G...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000016|[GCA, CAA, AAT, A...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000017|[GAC, ACT, CTC, T...|(82,[0,1,2,3,4,5,...|         1|\n",
      "|ALPH01000018|[AGA, GAC, ACT, C...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000019|[CTT, TTC, TCT, C...|(82,[0,1,2,3,4,5,...|         0|\n",
      "|ALPH01000020|[AGG, GGA, GAT, A...|(82,[0,1,2,3,4,5,...|         0|\n",
      "+------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        prediction|\n",
      "+-------+------------------+\n",
      "|  count|              1864|\n",
      "|   mean|0.3063304721030043|\n",
      "| stddev| 0.690432002592243|\n",
      "|    min|                 0|\n",
      "|    max|                 4|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_df.select(\"prediction\").describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_env",
   "language": "python",
   "name": "bio_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
